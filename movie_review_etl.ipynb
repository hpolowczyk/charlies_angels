{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMRF5QPj1bH9"
   },
   "outputs": [],
   "source": [
    "#Load dependenices\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pymongo\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jvomAOS1bIH"
   },
   "outputs": [],
   "source": [
    "# Import file containing imdb movie urls\n",
    "url_df = pd.read_csv('https://charlies-angels.s3.us-east-2.amazonaws.com/movie_urls.txt', names = [\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U1EJPVyl1bIP"
   },
   "outputs": [],
   "source": [
    "# Remove duplicates and reset index\n",
    "url_df = url_df.drop_duplicates()\n",
    "url_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# Add a column for Reviews\n",
    "url_df['reviews'] = ''\n",
    "\n",
    "# Add a column for IMDB Movie ID\n",
    "url_df['movie_id'] = url_df['url'].str.replace('/usercomments','').str.replace('http://www.imdb.com/title/','')\n",
    "\n",
    "# Update the URLs to what we want\n",
    "url_df['url'] = url_df['url'].str.replace('usercomments','reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "b6qpbZpxBMEo",
    "outputId": "876151a0-ee44-4975-ca6f-3b9fb327ac7c"
   },
   "outputs": [],
   "source": [
    "# Online Movie Database (OMDB) API URL including API Key and full plot\n",
    "api_url = \"http://www.omdbapi.com/?apikey=baee4093&plot=full\"\n",
    "\n",
    "movie_data = []\n",
    "\n",
    "# Using a for-loop...\n",
    "for movie_id in url_df['movie_id']:\n",
    "    # Use the movie_id to pull data from the API\n",
    "    try:\n",
    "        movie = requests.get(api_url + '&i=' + movie_id).json()\n",
    "        if movie['Response'] == \"True\":\n",
    "            movie_data.append(movie)\n",
    "    except JSONDecodeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJDT_m1iBPcz"
   },
   "outputs": [],
   "source": [
    "# Create a pandas Dataframe using movie_data\n",
    "omdb_df = pd.DataFrame(movie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xm_qCFeMBUhB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6018 entries in the movie_only dataframe.\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the dataframe for cleaner manipulation\n",
    "movie_only = omdb_df.copy()\n",
    "# Ensure that only movie titles are being used\n",
    "movie_only = movie_only[movie_only['Type'] == 'movie']\n",
    "# Drop any duplicates\n",
    "movie_only = movie_only.drop_duplicates(subset =\"imdbID\") \n",
    "# Drop any rating with N/A\n",
    "movie_only = movie_only[movie_only['imdbRating'] != 'N/A']\n",
    "# Convert the rating column to a numeric one\n",
    "movie_only['imdbRating'] = pd.to_numeric(movie_only['imdbRating'])\n",
    "\n",
    "# Check the number of entries\n",
    "unique_films = movie_only['imdbID'].nunique()\n",
    "print(f'There are {unique_films} entries in the movie_only dataframe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4NBz4-Q5FhZq"
   },
   "outputs": [],
   "source": [
    "# Merge both dataframes together using an inner join \n",
    "full_df = pd.merge(url_df, movie_only, left_on='movie_id', right_on='imdbID', how='inner')\n",
    "\n",
    "#Rename imdbRating to label, since Naive Bayes requires it\n",
    "full_df.rename(columns={'imdbRating' : 'label'}, inplace=True)\n",
    "# Clean up the columsn to ensure only the columns we need will be used\n",
    "full_df = full_df[['url', 'reviews', 'movie_id', 'Title', 'Year', 'Genre', 'Actors','Plot', 'Poster', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fM-aq_q1bIu"
   },
   "outputs": [],
   "source": [
    "# Retrieve reviews\n",
    "for idx in range(len(full_df)):\n",
    "    reviews = ''\n",
    "    \n",
    "    response = requests.get(full_df.loc[idx, 'url'])\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    results = soup.find_all('div', class_='text show-more__control')\n",
    "    \n",
    "    for result in results:\n",
    "        reviews = reviews + result.text + ' ' \n",
    "        #result.find('div', class_='text show-more__control').text.strip()\n",
    "    \n",
    "    full_df.loc[idx, ['reviews']] = reviews\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-WE7wG2NrmM"
   },
   "outputs": [],
   "source": [
    "# Remove any films with blank reviews\n",
    "full_df = full_df[full_df['reviews'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPv1NmVg1bIy"
   },
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rJ4K9HCt1bI2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1e53eddd688>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of dictionaries in order to efficiently insert into MongoDB\n",
    "movies_dict = full_df.to_dict('records')\n",
    "\n",
    "# MongoDB connection\n",
    "conn = 'mongodb+srv://general_user:charli3s_ang3ls@cluster0-tyboh.mongodb.net/movie_db?retryWrites=true&w=majority'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "# Declare the collection\n",
    "collection = client.movie_db.movie_reviews\n",
    "#Drop collection if it exists to prevent duplication\n",
    "collection.drop()  \n",
    "# Insert all of the documents into the collection\n",
    "collection.insert_many(movies_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPVW1XER7bIX"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
    "\n",
    "!pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pb_afKpE7bMs"
   },
   "outputs": [],
   "source": [
    "reviews_df = spark.createDataFrame(df)\n",
    "reviews_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-XXZnhXP9ukm"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "# Create a length column to be used as a future feature \n",
    "data_df = reviews_df.withColumn('length', length(reviews_df['reviews']))\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6Lj19ZS9uny"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "# Create all the features to the data set\n",
    "#pos_neg_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"reviews\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eQ6XLjkM9urx"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "# Create feature vectors\n",
    "#clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')\n",
    "clean_up = VectorAssembler(inputCols=['idf_token'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZx9Ky2o9uy0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(stages=[tokenizer, stopremove, hashingTF, idf, clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCQieBi79_DC"
   },
   "outputs": [],
   "source": [
    "# Fit and transform the pipeline\n",
    "cleaner = data_prep_pipeline.fit(data_df)\n",
    "cleaned = cleaner.transform(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tkqbc469_GG"
   },
   "outputs": [],
   "source": [
    "# Show label and resulting features\n",
    "cleaned.select(['label', 'features']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qzPHUyvs9_JQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "# Break data down into a training set and a testing set\n",
    "training, testing = cleaned.randomSplit([0.75, 0.25])\n",
    "\n",
    "# Create a Naive Bayes model and fit training data\n",
    "nb = NaiveBayes()\n",
    "predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61Q9IeN39_Pv"
   },
   "outputs": [],
   "source": [
    "# Tranform the model with the testing data\n",
    "test_results = predictor.transform(testing)\n",
    "test_results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quHdLvzd-mcZ"
   },
   "outputs": [],
   "source": [
    "# Use the Class Evaluator for a cleaner description\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcjNGq8r-mkG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "retrieve_movie_reviews.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
